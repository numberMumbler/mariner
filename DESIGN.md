# Mariner Design Document

1. [Overview](#overview)
1. [UI](#ui)
1. [System Design](#system-design)
1. [Data Design](#data-design)
1. [Testing](#testing)
1. [Monitoring](#monitoring)
1. [Security](#security)

## Overview

Mariner is designed to transform the way academic research is consumed by making it accessible and relevant to a broader audience. By providing clear, concise summaries of academic papers, Mariner eliminates the barrier of complex jargon and dense mathematical proofs, allowing users to stay abreast of the latest scientific breakthroughs tailored to their interests. This platform is not just for those unaccustomed to reading scholarly articles but also serves as a time-saving tool for seasoned academics who wish to quickly scan the latest research before delving into full papers for detailed study.

At its core, Mariner's mission is to democratize access to cutting-edge research and encourage a deeper understanding of the evolving trends across various disciplines. It achieves this through smart content curation, personalization, and a user-centric interface, making it possible for users to explore outside their expertise and connect disparate ideas, thus fostering creativity and innovation. Whether for a quick update on recent studies or exploring new interdisciplinary insights, Mariner is tailored to enhance discovery and learning in the ever-expanding universe of academic knowledge.

### Scope

The scope of Mariner is focused on delivering a personalized academic digest only from the arXiv repository. In its initial version, Mariner will cater to individual users by offering a tailored feed of research articles, where each user can bookmark or archive articles in their feed. The platform will present articles chronologically based on their publication date, supplemented with a customized summary and a link to the full text on the arXiv site.

- Mariner will initially aggregate content exclusively from arXiv, so users won't have access to papers from other academic journals or repositories.
- Users can select their interests from a predefined list, and these choices will dictate the content of the summaries they receive. The system lacks the capability for users to finely tune or define the nuances of their interest areas beyond this selection.
- Summaries generated by an LLM (Large Language Models) may not capture every detail accurately. For critical or detailed information, users are encouraged to consult the original papers.
- Once a user adjusts their interest preferences, only new articles added to the system will reflect these changes. Previously summarized articles in their feed will not be updated to align with new interests.
- Mariner does not provide a feature to directly share summaries or articles on social media platforms; users will need to manually post about articles if they wish to share them.

### Priorities

Use these to make decisions about trade-offs.

1. **Automation** The system should require minimal attention after initial development is complete
1. **Reliability** Things should work as promised, and the system should be able to quickly recover from any errors or outages
1. **Extensibility** The system should accommodate future features, data sources, use cases
1. **Simplicity** Use as little code as possible for the logic, limit options and customizations

### Key Use Cases

- As a user, I want to quickly review recently published articles, so I can keep up with new research that is coming out.
- As a user, I want to bookmark articles of interest, so that I can easily find and read them later in detail.
- As a user, I want to archive articles I'm not interested in, so that they no longer clutter my feed and I can focus on more relevant content.
- As a user, I want to adjust my interest areas, so that the summaries in my feed reflect my changing research interests and needs.
- As a system administrator, I want to monitor the system's health and performance, so that I can ensure it operates efficiently and effectively.
- As a content manager, I want to manage the list of interest areas, so that the system remains relevant and up-to-date with current research trends.
- As a system administrator, I want to manage user accounts, so that I can ensure only authorized users access the system and maintain data privacy and security.

## UI

![Review section mockup](MarinerUI.png)

The user's main page is a list of research articles, called the Review section. For each article, they see the title, authors, and a link to the source article (e.g. the entry on arXiv's website). There is a customized summary that highlights things the user might be interested in. The user might click on the link and read the full paper if they are interested. The user might also click the bookmark icon, so they can quickly find this article later. Most of the time, the user will click the archive icon, because the summary is enough for them to be aware.

Bookmarking an article flags it, but it stays where it is. Archiving an article moves it from the review feed to the Archived section. Users can view all bookmarked articles in the Bookmarked section.

## System Design

### System Context

```mermaid
C4Context
  title Mariner System Context Diagram

  Person(user, "User", "Uses Mariner to access personalized summaries of academic research")
  System_Boundary(mariner_system, "Mariner") {
    System(mariner, "Website", "Provides personalized research paper summaries")
    SystemDb(marinerDB, "MarinerDB", "Stores articles, summaries, and user preferences")
    System(summary_generator, "Summary Generator", "Generates custom summaries for each user")
    System(fetcher, "Fetcher", "Retrieves new articles from Article Sources")
  }
  System_Boundary(article_sources, "Article Sources") {
    System_Ext(arXiv, "arXiv", "Source of academic articles")
  }
  System_Ext(openai, "OpenAI API", "Provides AI-powered summarization services")

  Rel(user, mariner, "Interacts with")
  Rel(mariner, marinerDB, "Stores and retrieves user data, preferences, and summaries")
  Rel(fetcher, arXiv, "Fetches new articles from")
  Rel(fetcher, marinerDB, "Stores article metadata in")
  Rel(summary_generator, marinerDB, "Queries for articles to summarize")
  Rel(summary_generator, openai, "Uses for generating summaries")
  Rel(marinerDB, summary_generator, "Provides articles for summarization")
  Rel(summary_generator, marinerDB, "Stores generated summaries in")
  Rel(mariner, user, "Delivers summaries to")
```

- **User and Mariner**: Users interact with Mariner to view personalized summaries of academic research. They can bookmark articles of interest or archive those they are done reviewing.
- **Mariner and MarinerDB**: Mariner stores user data, preferences, and generated summaries in MarinerDB. It also retrieves this information to present personalized content to users.
- **Fetcher and arXiv**: The Fetcher component retrieves new articles from arXiv. It is designed to handle the high volume of articles published monthly, ensuring Mariner remains up-to-date with current research.
- **Fetcher and MarinerDB**: After fetching, Fetcher stores the article metadata, including Article ID, Title, Authors, Subject, Source URL, and Content URL, in MarinerDB.
- **Summary Generator and MarinerDB**: The Summary Generator queries MarinerDB to find articles that need summarizing. It ensures that each user gets custom summaries based on their interests.
- **Summary Generator and OpenAI API**: Utilizing the OpenAI API, the Summary Generator creates personalized summaries for the articles. This process involves downloading the article's content, summarizing it, and then deleting the downloaded files to manage storage efficiently.
- **MarinerDB and Summary Generator**: MarinerDB provides articles to the Summary Generator for summarization, ensuring that only new and relevant articles are processed for each user.
- **Mariner and User**: Finally, Mariner delivers these summaries to the user, maintaining a feed that is relevant, up-to-date, and tailored to the user's specified interests.

In Mariner, aggressive removal of archived summaries and limiting the user's review feed to the most recent articles help manage storage and ensure users are presented with the most relevant and current research, aligning with Mariner’s goals of providing timely and pertinent academic insights.

### Sequence Diagram

```mermaid
sequenceDiagram
    actor User
    participant Website as Mariner Website
    participant Fetcher
    participant MarinerDB as Mariner Database
    participant SummaryGenerator as Summary Generator
    participant OpenAI as OpenAI API
    participant ArXiv as arXiv

    User->>Website: Access Mariner
    loop Check for New Articles
        Fetcher->>ArXiv: Request new articles
        ArXiv->>Fetcher: Return new articles
        Fetcher->>MarinerDB: Store article metadata
    end
    loop Summary Generation
        MarinerDB->>SummaryGenerator: Provide articles needing summaries
        SummaryGenerator->>MarinerDB: Retrieve full articles
        SummaryGenerator->>OpenAI: Send text for summarization
        OpenAI->>SummaryGenerator: Return summaries
        SummaryGenerator->>MarinerDB: Store summaries
    end
    Website->>MarinerDB: Retrieve summaries for User
    Website->>User: Display summaries
```

- **User Interaction**: The user accesses the Mariner Website to view summaries or manage their interests.
- **Fetching New Articles**: The Fetcher periodically requests new articles from arXiv, which are then returned and stored in the Mariner Database.
- **Summary Generation**:
  - The Mariner Database provides articles that need summarizing to the Summary Generator.
  - The Summary Generator retrieves the full text of these articles from the Mariner Database.
  - It then sends this text to the OpenAI API for summarization.
  - The OpenAI API processes the text and returns the summaries to the Summary Generator.
  - The Summary Generator saves these summaries in the Mariner Database.
- **Display Summaries**: Finally, the Mariner Website retrieves the summaries from the Mariner Database and displays them to the user.

This sequence diagram provides a clear step-by-step visualization of how new articles are ingested and processed to generate summaries in the Mariner system, highlighting the interaction between different components of the system and external services.

### Front End Implementation

React.js is selected for the Mariner front end, leveraging its component-based structure to efficiently handle dynamic updates, such as real-time summary modifications. This choice supports streamlined development and future maintenance, reducing the need for ongoing adjustments and aligning with the goal of automation.

The multi-page architecture, as opposed to a single-page application, enhances initial load performance and system stability, ensuring reliability for end-users. Each section, like the review feed and bookmarked articles, is managed separately, facilitating quicker recovery from disruptions and contributing to overall system resilience.

This approach also allows for clear scalability and the integration of new functionalities, addressing the extensibility priority. As Mariner evolves, new sections or features can be added with minimal disruption to the existing infrastructure.

By adopting React.js and avoiding the complexities of SPA architecture, Mariner maintains simplicity in its front-end design. This ensures that the UI remains straightforward to update and manage, resonating with the simplicity priority by minimizing unnecessary code and focusing on core functionalities.

### Back End Implementation

Ruby on Rails is the chosen framework for Mariner's back end, capitalizing on its comprehensive set of features that streamline web application development. This choice supports a streamlined initial setup and future maintenance efficiency, fitting well with the need for a system that automates complex processes seamlessly. The framework's robustness and proven track record ensure the back end's reliability, particularly important as Mariner evolves and scales.

Mariner will operate as a monolith, simplifying development and reducing the complexities of deployment and operation. This single-instance model is sufficient for the expected user load during the closed beta and simplifies the transition to a load-balanced architecture when user demand increases. Such a setup allows for straightforward scalability and system enhancement without immediate architectural overhauls.

ArxivFetcher and Summary Generator will be integrated as background services within the Rails application, sharing models, logic, and configurations to promote code reuse and maintain consistency. ArxivFetcher will be scheduled as a cron job to run daily, ensuring up-to-date article retrieval without manual intervention. Summary Generator, designed for parallel processing, will be triggered periodically to manage workload efficiently, preparing for potential future scaling.

By embedding these components within the Rails ecosystem, Mariner benefits from a cohesive, manageable codebase that adheres to a unified deployment strategy. This approach allows for distinct, scalable operations of each component, aligning with Mariner’s growth and expansion goals, and laying a foundation for a robust, user-centric service.

### Database

Mariner uses MongoDB. A document database's flexible schema accommodates the varying structures of the content and user data. This adaptability is crucial for Mariner's evolving needs, allowing for straightforward updates to the database schema without significant overhauls. MongoDB's performance with large datasets ensures that Mariner can handle the growing volume of articles and user interactions efficiently.

While initially deployed as a single instance, With MongoDB's potential for redundancy and scaling, Mariner has a robust foundation for feature expansions and user growth.

## Data Design

### Data Model Design

#### 1. Articles Collection

- **\_id**: MongoDB provided unique identifier (can be used as the arXiv ID).
- **title**: String
- **authors**: Array of strings
- **abstract**: String
- **topics**: Array of strings
- **url**: String (URL to the arXiv entry)
- **submittedDate**: Date
- **status**: String (e.g., "RETRIEVED", "SUMMARIZED")
- **statusDate**: Date (date when the status was last updated)
- **summaries**: Array of objects containing:
  - **userId**: String or ObjectId (to link to a User collection)
  - **summaryText**: String
  - **relevanceScore**: Number
  - **interests**: Array of strings (topics or interests this summary caters to)

#### 2. Users Collection

- **\_id**: MongoDB provided unique identifier
- **name**: String
- **email**: String
- **interests**: Array of strings
- **actions**: Array of objects containing:
  - **articleId**: ObjectId (reference to Articles collection)
  - **action**: String (e.g., "FAVORITE", "ARCHIVE", "REMOVE")
  - **actionDate**: Date

### System Flow Integration

- When new papers are fetched from arXiv, they are added to the Articles collection with the status "RETRIEVED". The statusDate is set to the current date.
- Another process updates the status to "SUMMARIZED" after generating the summary, also updating the statusDate to reflect this change.
- Users can view, favorite, archive, or remove articles through the interface, which updates their actions in the Users collection.

### Design Justification and Extension

- **Scalability**: MongoDB’s flexible schema allows for easy addition of new fields or changes to existing structures, facilitating future expansions.
- **Performance**: By storing summaries within the Articles collection, you minimize the need for extensive joins or lookups, which suits MongoDB's document-oriented nature.
- **User-Specific Relevance**: Storing a relevance score and interests per summary allows for personalized content filtering without complex query logic.
- **Status Tracking**: The status and statusDate fields help track the lifecycle of an article's processing and identify bottlenecks or issues in the workflow.
- **User Actions**: Keeping user actions linked with articles in the Users collection simplifies user-specific operations and preferences management.

### Potential Extensions

- As the user base grows, you might want to introduce indexing on frequently queried fields (e.g., status, topics, userId, articleId) to enhance performance.
- For handling more complex workflows or a significant increase in data volume, consider introducing message queuing or stream processing systems for better load management and real-time processing capabilities.

This design provides a solid foundation that meets your current requirements while offering flexibility for future enhancements and scalability.

In the system described, the following queries are likely to be involved, categorized by their expected frequency of use and strategies for optimization.

### Most Frequent Queries

1. **Retrieving new entries for processing**

   - Query to find articles with a status of "RETRIEVED".
   - Used frequently as part of the automated process to update the status to "SUMMARIZED" after processing.
   - **Optimization**: Index on `status` to speed up retrieval of articles awaiting processing.

2. **Summarizing articles**

   - Query to fetch the full text of articles for summarization.
   - Runs frequently as part of the summary generation process.
   - **Optimization**: Since this will likely fetch the entire document, ensure efficient retrieval by having a good storage setup and potentially caching mechanisms for frequently accessed data.

3. **User-specific article retrieval**
   - Query to display articles and summaries based on user interests and actions (favorites, archives, removals).
   - Highly frequent as users interact with the system to read summaries.
   - **Optimization**: Index on `summaries.userId` and `actions.articleId` to quickly fetch articles related to a specific user. Consider also indexing `interests` if filtering by user interest is common.

### Occasionally Used Queries

1. **Status update for articles**

   - Updating the `status` and `statusDate` of an article after it has been processed.
   - Occurs less frequently, corresponding to the rate of article processing and summarization.
   - **Optimization**: While not accessed as often, ensuring the `status` field is indexed supports quick updates.

2. **User action updates (favorite, archive, remove)**
   - Inserting or updating user actions related to articles.
   - Occurs as users interact with the system, but less frequently than viewing articles.
   - **Optimization**: Using an array of actions within the Users collection can minimize the complexity of these queries. Indexing on `actions.articleId` can speed up updates and retrievals of user actions.

### Rare Queries

1. **Periodic maintenance tasks**
   - Queries for system maintenance, such as cleaning up old data or analyzing usage patterns.
   - Very infrequent, potentially running as background tasks during off-peak hours.
   - **Optimization**: These tasks usually don’t require immediate execution, so they can be scheduled during low-load periods to minimize impact on system performance.

### Optimization Strategies

- **Indexing**: Create indexes on the most frequently queried fields to reduce lookup times. This is crucial for fields like `status`, `userId`, and `articleId`.
- **Balanced Schema Design**: While MongoDB is schema-less, designing a balanced document structure that minimizes unnecessary data duplication can improve performance.
- **Caching**: Implement caching for frequently accessed data, especially for articles that are viewed and summarized repeatedly.
- **Background Processing**: Schedule maintenance and low-priority tasks during off-peak hours to avoid impacting system performance during high-usage periods.

By prioritizing these optimizations, the system can handle frequent operations efficiently while retaining the flexibility to accommodate future extensions and increased workload.

## Testing

### Unit Testing

For unit testing in Ruby, the most common framework is RSpec. It provides a rich and flexible toolset for testing Ruby applications and is well-suited for behavior-driven development (BDD).

- **Code Coverage**: While 100% code coverage is ideal, it might not always be practical, especially for areas heavily interfacing with external services or the database. Aim for a code coverage target of around 80-90%, prioritizing critical paths and business logic. Tools like SimpleCov can be used with RSpec to track code coverage.
- **Mocking and Stubbing**: Given Mariner’s reliance on external services, use mocking and stubbing to simulate external API calls within unit tests. This approach ensures tests remain fast and reliable without depending on external systems.

### Integration Testing

Integration tests should focus on workflows that are critical to Mariner’s operation, specifically the interaction between components and with external systems:

- **Summarization Workflow**: Test the entire process from fetching articles from the ArXiv database, summarizing them with the GPT service, and storing the results in the Mariner Database.
- **User Interaction Workflow**: Test how the system handles user interactions, such as fetching personalized content, updating preferences, and displaying summaries.
- **Database Integration**: Ensure that the application correctly interacts with MongoDB, including reading and writing data, and handling database errors gracefully.

For integration testing, tools like Capybara can be used alongside RSpec in Ruby, providing a way to simulate user interactions and test the integration of various system components.

### CI/CD and Deployment Testing

For continuous integration and deployment (CI/CD), a suite of tests should be in place to ensure that new changes don't break existing functionality and that the code is always production-ready:

- **Automated Test Suite**: Configure a CI tool (like Jenkins, GitHub Actions, or GitLab CI) to run the full suite of unit and integration tests on every commit or pull request. This setup helps in identifying issues early in the development cycle.
- **Deployment Pipeline Tests**: Include tests in the deployment pipeline to verify that the application can be successfully deployed to a staging or production environment. This might include smoke tests to quickly check the health and basic functionality of the deployed application.
- **Performance and Load Testing**: Consider incorporating basic performance and load testing in the CI/CD pipeline, especially for critical paths that could be impacted by scalability issues.

### Best Practices

- **Test Automation**: Automate as much of the testing process as possible to reduce manual effort and increase reliability.
- **Continuous Testing**: Integrate testing into the continuous integration process to ensure that issues are detected and addressed promptly.
- **Quality Gates**: Establish quality gates in the CI/CD pipeline, such as code coverage thresholds or mandatory code review approvals, to ensure that only high-quality code is deployed to production.

By adhering to these testing practices, Mariner can maintain high standards of quality and reliability, facilitating a smooth and efficient development lifecycle.

## Monitoring

### Overview

Effective monitoring and alerting are crucial for maintaining the reliability and performance of Mariner. Given the application's architecture, including a Ruby on Rails web application, a MongoDB database, and interactions with external systems, we need to establish a basic monitoring setup that can be expanded in the future.

### Key Components to Monitor

#### 1. **Application Performance (Rails)**

- **Response Times**: Monitor the average time it takes for the application to respond to requests, which can help identify performance degradations.
- **Error Rates**: Track the frequency of application errors or exceptions to catch and address issues promptly.
- **Request Throughput**: Measure the number of requests handled by the application over time to understand traffic patterns and potential stress points.

#### 2. **Database Performance (MongoDB)**

- **Query Performance**: Monitor the execution time of database queries to identify slow queries that may need optimization.
- **Connection Counts**: Keep track of the number of active connections to the database to prevent connection overflows.
- **Resource Utilization**: Monitor the database server's CPU, memory, and disk usage to detect potential bottlenecks or resource constraints.

#### 3. **External Systems Interactions**

- **API Response Times**: Track the response times of external systems (ArXiv and GPT Service) to detect latency issues or unavailability.
- **API Error Rates**: Monitor the error rates of API calls to external services to quickly identify integration issues or service disruptions.

### Alerting

Initially, setting up a simple status page that provides real-time visibility into the system's health and performance metrics can be sufficient. This page should display the status of key components and any notable issues detected by the monitoring system.

For future-proofing the monitoring and alerting setup, consider using tools that allow for easy escalation. For example, if a critical performance metric exceeds a certain threshold, the system should support triggering alerts through emails or push notifications.

## Security

TODO: review OWASP top 10

### Secrets Management

- **Environment Variables**: While using `.env` files and environment variables is a good practice, ensure that `.env` files are not committed to version control systems by including them in `.gitignore`.
- **Secure Storage**: Use secrets management tools (like HashiCorp Vault, AWS Secrets Manager, or environment-specific secrets management in hosting platforms) to securely store and access secrets like API keys, database credentials, and other sensitive configuration details.
- **Access Controls**: Limit access to environment variables and configuration files to only parts of the system and personnel that need them.

### Application

TODO: what does Rails provide out of the box?

### LLM security
